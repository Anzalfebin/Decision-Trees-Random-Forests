# Task 5 - Decision Trees and Random Forests

This project is part of an AI & ML Internship task focused on tree-based machine learning models.

## ðŸ“Œ Objective
- Learn Decision Trees and Random Forests
- Understand overfitting and how to control it
- Compare single tree vs ensemble learning
- Interpret feature importance
- Evaluate models using cross-validation

---

## ðŸ“‚ Dataset
Heart Disease Dataset (from Kaggle)

---

## ðŸ›  Tools Used
- Python
- Pandas
- NumPy
- Matplotlib
- Scikit-learn

---

## ðŸš€ Steps Performed

### 1. Decision Tree Classifier
- Trained a decision tree model
- Evaluated accuracy
- Visualized the tree structure

### 2. Overfitting Control
- Limited tree depth using `max_depth`
- Observed improved generalization

### 3. Random Forest
- Trained ensemble model with multiple trees
- Compared accuracy with decision tree
- Random Forest performed better

### 4. Feature Importance
- Extracted feature importance values
- Visualized most influential features

### 5. Cross Validation
- Used 5-fold cross validation
- Evaluated model stability

---

## ðŸ“Š Results
- Decision Tree Accuracy: Good but prone to overfitting
- Random Forest Accuracy: Higher and more stable
- Cross-validation confirmed model reliability

---

## ðŸ§  Key Learnings
- Decision Trees are easy to interpret but overfit easily
- Random Forest reduces overfitting using bagging
- Feature importance helps understand model decisions
- Cross-validation ensures reliable evaluation

---

## ðŸ“Ž Submission
Upload this project to GitHub and submit the repository link.
